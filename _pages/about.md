---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hey stranger, this is Sadra. I‚Äôm currently studying my PhD in Computer Science at USC, working at the intersection of Human-Computer Interaction (HCI) and Natural Language Processing (NLP)‚Äîi.e., making LLMs better friends of humans.
My current research focuses on helping people make better decisions using LLMs.
On the side (and honestly, all the time), I build and maintain scientific software tools with a great team of open-source enthusiasts.
I‚Äôm always looking for ways to make technology and science more accessible, and fun‚Äîbelieving that open-source software is an ideal contribution to scientific communities that value transparency and reproducibility.
I enjoy watching movies and hunting for new places whenever I travel in my free time.
I'm always curious to meet new people and hear about their journeys, so shoot me an email or DM me on any social media!


<details>
<summary>**CS PhD @ USC ‚úåÔ∏è**</summary>
Peek a boo!
</details>

The main problem I'm trying to solve is the integration of AI systems into human workflows‚Äîspecifically, answering the question: "What is the core part of a task that AI cannot do, and how can AI assist humans in doing that?"
Helping humans tackle the hardest parts of their jobs‚Äîwith AI as a consultant‚Äîis the overarching meta-goal of my current research.
To address this, I've explored several domains where large language models (LLMs) have been introduced but face full-integration challenges. These include:
+ software developers trusting code agents for programming,
+ strategic decision-making in the board game Diplomacy, and
+ patients navigating conflicting medical advice.

I'm currently in my second year and looking forward to exploring more domains to develop a taxonomy of these challenges and a framework that identifies the right interaction patterns and integration points for AI.
Throughout this journey, I've had the great opportunity to work with the Adaptive Computing Experience (ACE) Lab (Souti Chattopadhyay‚Äôs lab @ GCS) and [CUTE LAB NAME] (Jonathan May‚Äôs lab @ ISI).
You can find my publications below:

<details>
  <summary>[ICSE25] *Trust dynamics in AI-assisted development: Definitions, factors, and implications,* **Sadra Sabouri**, Philipp Eibl, Xinyi Zhou, Morteza Ziyadi, Nenad Medvidovic, Lars Lindemann, Souti Chattopadhyay</summary>
  <a href="https://www.amazon.science/publications/trust-dynamics-in-ai-assisted-development-definitions-factors-and-implications" style="text-decoration: none;"><div style="display: inline-block;padding: 6px 12px;background-color: #007BFF;color: white;border-radius: 4px;font-size: 14px;text-align: center;cursor: pointer;">PDF</div></a>
  
  Software developers increasingly rely on AI code generation utilities. To ensure that ‚Äúgood‚Äù code is accepted into the code base and ‚Äúbad‚Äù code is rejected, developers must know when to trust an AI suggestion. Understanding how developers build this intuition is crucial to enhancing developer-AI collaborative programming. In this paper, we seek to understand how developers (1) define and (2) evaluate the trustworthiness of a code suggestion and (3) how trust evolves when using AI code assistants. To answer these questions, we conducted a mixed method study consisting of an in-depth exploratory survey with (n= 29) developers followed by an observation study (n= 10). We found that comprehensibility and perceived correctness were the most frequently used factors to evaluate code suggestion trustworthiness. However, the gap in developers‚Äô definition and evaluation of trust points to a lack of support for evaluating trustworthy code in real-time. We also found that developers often alter their trust decisions, keeping only 52% of original suggestions. Based on these findings, we extracted four guidelines to enhance developer-AI interactions. We validated the guidelines through a survey with (n= 7) domain experts and survey members (n= 8). We discuss the validated guidelines, how to apply them, and tools to help adopt them.
</details>
<details>
  <summary>[ACL25] *ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations,* Brihi Joshi, Keyu He, Sahana Ramnath, **Sadra Sabouri**, Kaitlyn Zhou, Souti Chattopadhyay, Swabha Swayamdipta, Xiang Ren</summary>
  <span class="link-block"><a href="https://arxiv.org/pdf/2506.14200" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span></a>
  <span class="link-block"><a href="https://github.com/INK-USC/ELI-Why" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fab fa-github"></i></span><span>Code</span></a></span>
  <span class="link-block"><a href="https://huggingface.co/collections/INK-USC/eli-why-6849086c86556f7a2dd7c686" class="external-link button is-normal is-rounded is-dark"><span class="icon"><img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em;"></span><span>Data</span></a></span>
  
  Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.
</details>

Always happy to chat, collaborate, or just hear what you're working on; feel free to reach out!


<!-- <details>
<summary>**Open World Developer üåê**</summary>
In my free time, I become an open-source software developer! I'm an advocate for collaboration and shared knowledge. You'll find more about my open-source activities on my GitHub profile. Following that I co-founded [OpenSciLab](https://openscilab.com/) as a community for open science.

</details>

### News

Jan 2025: My paper [Trust dynamics in AI-assisted development: Definitions, factors, and implications](https://www.amazon.science/publications/trust-dynamics-in-ai-assisted-development-definitions-factors-and-implications) got accepted into International Conference on Software Engineering (ICSE) 2025. I will present my work remotely in searly May.

Sep 2024: I was awarded a [Trelis AI Grant](https://trelis.com/trelis-ai-grants/) for developing a RESTful API for PyCM, enhancing accessibility to machine learning statistical post-processing tools. -->
